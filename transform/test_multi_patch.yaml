# config for logger
logger:
  level: INFO
  name: IAAPyTorch
# config for trainer
trainer:
  name: ClassificationTrainer
  params:
    run_dir: runs
    run_id: attention-based-distribution
    epoch: 120
    gpu: !!python/list [ 0,1,2,3,4 ]
    #    gpu: !!python/list [ 2, 3 ]
    log_every: 5
    #    num_workers: 8

    seed: 1234
    #required learning rate for NIMA
    base_lr: 5.0e-3
    head_lr: 1.0e-2
# config for dataset
dataset:
  name: AVADatasetWithIndex
  params:
    root: /mnt/cephfs/dataset/AVA_dataset
    base_dataset: AVAStyleClassificationDataset
# dataloader (Dataloader or MultiPatchDataloader)
train_dataloader:
  name: MultiPatchDataloader
  params:
    batch_size: 256
    num_workers: 6
    shuffle: True
    pin_memory: True
    collate_fn: sequence_multi_patch_collate
# dataloader (Dataloader or MultiPatchDataloader)
test_dataloader:
  name: Dataloader
  params:
    batch_size: 256
    num_workers: 6
    shuffle: True
    pin_memory: True
# config for model
model:
  name: NIMA
  params:
    base_name: vgg16
    pretrained: True
    #pool_window: !!python/list [ 2, 2 ]
    #fc_dims: !!python/list [ 256, 128 ]
    num_classes: 2
# config for loss function
loss:
  name: AdaptiveAttentionBasedLoss
  params:
    r: 2.0
    b: 2
# config for optimizer
optimizer:
  name: SGD
  params:
    #    lr: 0.001
    lr: 0.005
    weight_decay: 0.0001
    momentum: 0.9
# config for lr scheduler
lr_scheduler:
  name: WarmupLR
  params:
    warmup_steps: 10

# config for metric
metric:
  name: AccuracyFromDistribution
# config for training transformation
train_transforms:
  - name: Scale
    params:
      size: 256
  - name: CropPatches
    params:
      size: 224
      num_patches: 5
  - name: MultiPatchFlip
    params:
      p: 0.5
  - name: MultiPatchToTensor
# config for test transformation
test_transforms:
  - name: Scale
    params:
      size: 256
  - name: CenterCrop
    params:
      size: 224
  - name: ToTensor
  - name: Normalize
    params:
      mean: !!python/list [ 0.485, 0.456, 0.406 ]
      std: !!python/list [ 0.229, 0.224, 0.225 ]
